<!-- PROJECT BANNER --> <p align="center"> <img src="https://dummyimage.com/1200x250/0d1117/ffffff&text=Real-Time+Anomaly+Detection+System" alt="Project Banner"/> </p> <h1 align="center">âš¡ Real-Time Anomaly Detection System</h1> <p align="center"> <strong>Apache Kafka Â· PySpark Streaming Â· Machine Learning Â· PostgreSQL Â· Grafana</strong> </p> <p align="center"> <img src="https://img.shields.io/badge/Build-Passing-brightgreen?style=flat-square"/> <img src="https://img.shields.io/badge/PySpark-3.5-orange?style=flat-square"/> <img src="https://img.shields.io/badge/Kafka-Streaming-black?style=flat-square"/> <img src="https://img.shields.io/badge/Python-3.10-blue?style=flat-square"/> <img src="https://img.shields.io/badge/License-MIT-yellow?style=flat-square"/> </p>
ğŸ“ Overview

This project implements a real-time anomaly & fraud detection pipeline using modern data engineering and machine learning technologies. It simulates transactions, streams them via Kafka, scores them in PySpark using an Isolation Forest model, stores results in PostgreSQL, and visualizes anomalies via Grafana.

Perfect for:
âœ” Real-time streaming ML
âœ” Fraud analytics
âœ” Kafkaâ€“Spark pipelines
âœ” Data engineering portfolio projects

## ğŸš€ Architecture

```text
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚  Transaction Producer â”‚
               â”‚      (Python + Faker) â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                  Kafka                    â”‚
   â”‚        Topic: transactions_raw            â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      PySpark Streaming       â”‚
        â”‚  - Parse JSON                â”‚
        â”‚  - Build ML features         â”‚
        â”‚  - Apply Isolation Forest    â”‚
        â”‚  - Determine anomalies       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚             PostgreSQL            â”‚
       â”‚  Tables:                          â”‚
       â”‚   - transactions_scored           â”‚
       â”‚   - anomalies                     â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚             Grafana             â”‚
         â”‚    Real-time anomaly dashboards â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```





## ğŸ“ Project Structure
```text
realtime-anomaly-detection/
â”‚
â”œâ”€â”€ producer/
â”‚   â””â”€â”€ transaction_producer.py
â”‚
â”œâ”€â”€ streaming/
â”‚   â””â”€â”€ stream_processor.py
â”‚
â”œâ”€â”€ ml/
â”‚   â”œâ”€â”€ prepare_data.py
â”‚   â”œâ”€â”€ features.py
â”‚   â””â”€â”€ train_model.py
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ isolation_forest.pkl
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ transactions_log.jsonl
â”‚   â””â”€â”€ checkpoints/
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

## âš™ï¸ Setup & Installation
1ï¸âƒ£ Clone the repository
git clone https://github.com/YOUR_USERNAME/realtime-anomaly-detection.git
cd realtime-anomaly-detection

2ï¸âƒ£ Create & activate virtual environment
python -m venv venv
venv\Scripts\activate         # Windows
source venv/bin/activate     # macOS/Linux

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

4ï¸âƒ£ Start Kafka + PostgreSQL using Docker
docker-compose up -d

## ğŸ§ª Generate Training Data
# 1ï¸âƒ£ Run the Transaction Generator

This script simulates real-time transactions and writes them as JSONL for training.
```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transaction Generator (Py)  â”‚
â”‚  â€¢ Faker simulated data     â”‚
â”‚  â€¢ Normal + anomalous tx    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
      data/transactions_log.jsonl
```


# Run:

python producer/transaction_producer.py
```text

ğŸ“Š Step 2 â€” Convert JSONL â†’ Parquet

Convert raw logs to Parquet for ML efficiency.

JSONL â”€â”€â”€â”€â”€â–¶ Parquet
```


# Run:

python ml/prepare_data.py
```text

ğŸ¤– Step 3 â€” Train the ML Model (Isolation Forest)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     ML Training Pipeline           â”‚
â”‚  â€¢ Load Parquet                    â”‚
â”‚  â€¢ Build features                  â”‚
â”‚  â€¢ IsolationForest anomaly model   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â–¼
        models/isolation_forest.pkl
```


# Run training:

python ml/train_model.py

# ğŸ”¥ Run the Real-Time Streaming Job

This launches the PySpark pipeline that does live anomaly detection.
```text
Kafka Topic: transactions_raw
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PySpark Streaming Job       â”‚
â”‚  â€¢ Parse JSON               â”‚
â”‚  â€¢ Build features           â”‚
â”‚  â€¢ Apply ML model           â”‚
â”‚  â€¢ Insert results â†’ SQL     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
      PostgreSQL: anomalies table
```

# Start streaming:

python streaming/stream_processor.py

The job performs:

âœ“ Reads live events from Kafka

âœ“ Scores transactions using Isolation Forest

âœ“ Writes outputs to PostgreSQL

âœ“ Exposes anomaly metrics for Grafana

## ğŸ“ˆ Grafana Dashboards

<img width="900" height="600" alt="image" src="https://github.com/user-attachments/assets/60a5838d-be84-4c91-a92a-6a2873e3c1a0" />


Access Grafana:

ğŸ‘‰ http://localhost:3000

Login: admin / admin

Example PostgreSQL query:

SELECT timestamp, is_anomaly
FROM anomalies
ORDER BY timestamp;


## ğŸ§  Technologies Used
```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Component         â”‚ Technology                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Streaming         â”‚ Apache Kafka                                  â”‚
â”‚ Processing        â”‚ PySpark Structured Streaming                  â”‚
â”‚ Machine Learning  â”‚ Isolation Forest (scikit-learn)               â”‚
â”‚ Storage           â”‚ PostgreSQL                                    â”‚
â”‚ Visualization     â”‚ Grafana                                       â”‚
â”‚ Deployment        â”‚ Docker Compose                                â”‚
â”‚ Scripting         â”‚ Python                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
