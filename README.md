

â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—

â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘

â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘

â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘

â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘

â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•   â•šâ•â•   â•šâ•â•â•šâ•â•     â•šâ•â•



&nbsp;     REAL-TIME ANOMALY \& FRAUD DETECTION PIPELINE

&nbsp; Apache Kafka â€¢ PySpark Streaming â€¢ ML â€¢ PostgreSQL â€¢ Grafana

ğŸš€ Real-Time Anomaly Detection System

Kafka â†’ PySpark Streaming â†’ ML Model â†’ PostgreSQL â†’ Grafana Dashboards





















ğŸ“Œ Overview



This project is a real-time anomaly \& fraud detection system that simulates financial transactions, processes them through a streaming ML pipeline, stores results in PostgreSQL, and visualizes anomalies in Grafana.

\# Real-Time Transaction Anomaly Detection



End-to-end demo of a real-time fraud / anomaly detection pipeline using:



\- \*\*Kafka\*\* â€“ streaming transaction events  

\- \*\*Python / scikit-learn\*\* â€“ feature engineering \& Isolation Forest model  

\- \*\*Streaming processor\*\* (PySpark or pure Python) â€“ score events in real time  

\- \*\*PostgreSQL\*\* â€“ store scored transactions \& anomalies  

\- \*\*Grafana\*\* â€“ dashboards on top of PostgreSQL



The goal: show how you can go from \*\*raw streaming events â†’ ML scores â†’ live monitoring\*\*.



---



\## Architecture



&nbsp;              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

&nbsp;              â”‚  Transaction        â”‚

&nbsp;              â”‚    Producer         â”‚

&nbsp;              â”‚ (Python + Faker)    â”‚

&nbsp;              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

&nbsp;                        â”‚ JSON events

&nbsp;                        â–¼

&nbsp;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

&nbsp;â”‚                         Kafka                             â”‚

&nbsp;â”‚                   Topic: transactions\_raw                 â”‚

&nbsp;â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

&nbsp;                        â–¼

&nbsp;            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

&nbsp;            â”‚    PySpark Streaming     â”‚

&nbsp;            â”‚  - Deserialize JSON      â”‚

&nbsp;            â”‚  - Build ML Features     â”‚

&nbsp;            â”‚  - Apply IsolationForest â”‚

&nbsp;            â”‚  - Score transactions    â”‚

&nbsp;            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

&nbsp;                       â”‚ writes via JDBC

&nbsp;                       â–¼

&nbsp;        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

&nbsp;        â”‚            PostgreSQL            â”‚

&nbsp;        â”‚ tables:                          â”‚

&nbsp;        â”‚  - transactions\_scored           â”‚

&nbsp;        â”‚  - anomalies                     â”‚

&nbsp;        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

&nbsp;                           â–¼

&nbsp;             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

&nbsp;             â”‚          Grafana          â”‚

&nbsp;             â”‚ Real-time dashboards       â”‚

&nbsp;             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜



\## Project Structure Repository layout



realtime-anomaly-detection/

â”‚

â”œâ”€â”€ streaming/

â”‚   â””â”€â”€ stream\_processor.py         # PySpark streaming job

â”‚

â”œâ”€â”€ producer/

â”‚   â””â”€â”€ transaction\_producer.py     # Kafka data generator

â”‚

â”œâ”€â”€ ml/

â”‚   â”œâ”€â”€ prepare\_data.py             # convert JSONL â†’ Parquet

â”‚   â”œâ”€â”€ features.py                 # feature engineering

â”‚   â””â”€â”€ train\_model.py              # train Isolation Forest

â”‚

â”œâ”€â”€ models/

â”‚   â””â”€â”€ isolation\_forest.pkl        # saved ML model

â”‚

â”œâ”€â”€ data/

â”‚   â”œâ”€â”€ transactions\_log.jsonl      # raw training data

â”‚   â”œâ”€â”€ transactions.parquet

â”‚   â””â”€â”€ checkpoints/                # Spark checkpoints

â”‚

â”œâ”€â”€ docker-compose.yml              # Kafka + Zookeeper + PostgreSQL

â”œâ”€â”€ requirements.txt

â”œâ”€â”€ .gitignore

â”œâ”€â”€ LICENSE

â””â”€â”€ README.md



ğŸ› ï¸ Installation \& Setup

1ï¸âƒ£ Clone the repo

git clone https://github.com/YOUR\_USERNAME/realtime-anomaly-detection.git

cd realtime-anomaly-detection



2ï¸âƒ£ Create virtual environment

python -m venv venv

source venv/bin/activate       # Linux/Mac

venv\\Scripts\\activate          # Windows



3ï¸âƒ£ Install dependencies

pip install -r requirements.txt



4ï¸âƒ£ Start Kafka + Zookeeper + PostgreSQL

docker-compose up -d



ğŸ“ˆ 5ï¸âƒ£ Generate Training Data



Run:



python producer/transaction\_producer.py





This generates:



Realistic transactions



5% anomalies



Saves to data/transactions\_log.jsonl



Stop when you have enough data.



ğŸ§  6ï¸âƒ£ Convert JSON â†’ Parquet

python ml/prepare\_data.py



ğŸ¤– 7ï¸âƒ£ Train Isolation Forest Model

python ml/train\_model.py





Outputs:



Model saved to models/isolation\_forest.pkl



ğŸ”¥ 8ï¸âƒ£ Start Streaming Job

python streaming/stream\_processor.py





This will:



Read Kafka events



Build features



Score anomaly score



Insert results into PostgreSQL



ğŸ“Š 9ï¸âƒ£ Grafana Dashboards



Open browser â†’ http://localhost:3000



Login:



user: admin



password: admin



Add PostgreSQL datasource



Build dashboards using queries:



Example: Count anomalies

SELECT

&nbsp; timestamp AS time,

&nbsp; is\_anomaly

FROM anomalies

ORDER BY time;



